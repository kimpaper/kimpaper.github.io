<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>spark on 기록 저장소</title>
    <link>https://kimpaper.github.io/kimpaper.github.io/tags/spark/</link>
    <description>Recent content in spark on 기록 저장소</description>
    <image>
      <url>https://kimpaper.github.io/kimpaper.github.io/papermod-cover.png</url>
      <link>https://kimpaper.github.io/kimpaper.github.io/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 08 Jun 2016 15:32:00 +0000</lastBuildDate><atom:link href="https://kimpaper.github.io/kimpaper.github.io/tags/spark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>restful api 서버에서 평균 응답시간, 호출횟수, Min, Max 구하기</title>
      <link>https://kimpaper.github.io/kimpaper.github.io/2016/06/08/apachelog/</link>
      <pubDate>Wed, 08 Jun 2016 15:32:00 +0000</pubDate>
      
      <guid>https://kimpaper.github.io/kimpaper.github.io/2016/06/08/apachelog/</guid>
      <description>app에서 데이타 통신을하는 api서버가 있다
각 인터페이스별 평균 응답시간을 아파치 로그를 활용하여 구해봤다
 서버는 apache+tomcat, spring으로 구현한 서버이다
 hadoop과 spark, python 설정은
Python and Spark로 로그 파일 분석 (with hadoop) 을 참고 하자
1. 아파치 TransferLog 로그파일에 응답 시간 남기기 우선 분석하기 전에 아파치 로그에 응답 시간을 추가로 기록하도록 하자
/etc/httpd/conf.d/ssl.conf 경로에서 아래를 편집했다.
물론. 설정 파일이 있는 경로와 이름은 서버마다 틀릴 수 있다
1 2 3 4 5 6  &amp;lt;VirtualHost _default_:443&amp;gt; .</description>
    </item>
    
    <item>
      <title>Python and Spark로 로그 파일 분석 (with hadoop)</title>
      <link>https://kimpaper.github.io/kimpaper.github.io/posts/opensource/2016-05-30-spark-hadoop/</link>
      <pubDate>Mon, 30 May 2016 16:35:00 +0000</pubDate>
      
      <guid>https://kimpaper.github.io/kimpaper.github.io/posts/opensource/2016-05-30-spark-hadoop/</guid>
      <description>Spark를 이용한 파일 분석  spark도 잘 모르고 hadoop도 잘 모르는 상태에서 진행해서 틀린 부분이 있을 것이다.
참고로 OSX에서 진행된 작업이다.
 설정 1. Hadoop를 설치 하고 실행한다 2. hdfs상에 파일을 올린다. 1 2  cd /logs hdfs dfs -put test.log /input/   아래와 같이 파일 브라우징이 가능하다 아래에서 올라간 파일을 확인! http://localhost:50070/explorer.html#/input
3. spark의 python 커맨드 테스트.. $SPARK_HOME/bin/pyspark 하둡을 켜고 pyspark를 실행하면 아래와 같이 나온다
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  Python 2.</description>
    </item>
    
  </channel>
</rss>
