---
layout: post
title: 하둡 스프링 연동 테스트2 - hadoop 2.6.x with spring 4.0 (MapReduce WordCount example)
date: '2015-04-15T00:39:00.002-07:00'
author: 페이퍼
tags:
- hadoop
- spring
modified_time: '2015-10-08T02:52:24.063-07:00'
blogger_id: tag:blogger.com,1999:blog-335715462918866001.post-2562697657835486790
blogger_orig_url: http://kimpaper.blogspot.com/2015/04/2-hadoop-26x-with-spring-40-mapreduce.html
---

context-hadoop.xml에 아래 내용 추가. <br /><pre><code><br /><hdp:configuration id="hdConf"><br />    fs.default.name=hdfs://localhost:9000<br /></hdp:configuration><br /><br /><hdp:job id="wordCountJob"<br />        input-path="/input/"<br />        output-path="/output/"<br />        configuration-ref="hdConf"<br />        mapper="delim.app.service.WordCount$TokenizerMapper"<br />        reducer="delim.app.service.WordCount$IntSumReducer"<br />        ><br /></hdp:job><br /><br /><hdp:job-runner id="wordCountJobRunner" job-ref="wordCountJob" run-at-startup="false"><br /></hdp:job-runner><br /><br /></code></pre><br /><br />WordCount.java<br /><pre><code class="noxml"><br />import org.apache.hadoop.io.IntWritable;<br />import org.apache.hadoop.io.Text;<br />import org.apache.hadoop.mapreduce.Mapper;<br />import org.apache.hadoop.mapreduce.Reducer;<br />import org.slf4j.Logger;<br />import org.slf4j.LoggerFactory;<br /><br />import java.io.IOException;<br />import java.util.StringTokenizer;<br /><br />public class WordCount {<br />    private static final Logger logger = LoggerFactory.getLogger(WordCount.class);<br /><br />    public static class TokenizerMapper<br />            extends Mapper&lt;Object, Text, Text, IntWritable> {<br /><br />        private final static IntWritable one = new IntWritable(1);<br />        private Text word = new Text();<br /><br />        @Override<br />        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {<br />            logger.info("map key={}, value={}", key, value);<br /><br />            StringTokenizer itr = new StringTokenizer(value.toString());<br />            while (itr.hasMoreTokens()) {<br />                word.set(itr.nextToken());<br />                context.write(word, one);<br />            }<br />        }<br />    }<br /><br /><br />    public static class IntSumReducer<br />            extends Reducer&lt;Text, IntWritable, Text, IntWritable> {<br />        private IntWritable result = new IntWritable();<br /><br />        @Override<br />        public void reduce(Text key, Iterable&lt;IntWritable> values, Context context) throws IOException, InterruptedException {<br />            logger.info("reduce key={}", key);<br /><br />            int sum = 0;<br />            for (IntWritable val : values) {<br />                sum += val.get();<br />            }<br />            result.set(sum);<br />            context.write(key, result);<br />        }<br />    }<br />}<br /><br /></code></pre>Test.java  <br /><pre><code><br />@Autowired<br />private org.apache.hadoop.conf.Configuration hdConf;<br /><br />@Autowired<br />private JobRunner wordCountJobRunner;<br /><br />@Before<br />public void beforeCopyFile() throws IOException {<br />    String file = "/Users/paper/Desktop/4/14/debug.2015-04-09.log";<br /><br />    Path srcFilePath = new Path(file);<br />    Path dstFilePath = new Path("/input/debug.2015-04-09.log");<br /><br />    FileSystem hdfs = FileSystem.get(dstFilePath.toUri(), hdConf);<br />    hdfs.copyFromLocalFile(false, true, srcFilePath, dstFilePath);<br /><br />    hdfs.delete(new Path("/output/"), true);<br />}<br /><br />@Test<br />public void testRunJob() throws Exception {<br />    wordCountJobRunner.call();<br />}<br /><br /></code></pre><br />1. Before를 통하여 로컬에 있는 debug.log 파일을 hdfs에 카피 해놓는다. <br /><br />2. Job을 실행한다.<br /><br />3. 실행하면 debug.log 파일을 line단위로 읽어들이는걸 확인 할 수 있다. (WordCount$TokenizerMapper)